#!/bin/tcsh

#SBATCH -p GPU
#requesting one node
#SBATCH -N1
#requesting 12 cpus
#SBATCH -n12
#requesting 1 GPU
#        If you want to insist on using the XX GPU (XX=v100 or k80), then change this to
#        #SBATCH --gres=gpu:XX:1
#SBATCH --gres=gpu:1
#SBATCH --job-name=pkdgrav3
#SBATCH --mail-user=lorne.whiteway@star.ucl.ac.uk
#SBATCH --mail-type=ALL
#SBATCH --time=16-00:00:00
#SBATCH --mem=128000


## Call this job script using syntax like this:
## sbatch --export=ALL,experiment_name='gpu_1024_1024_1000' cuda_job_script

## To go to the GPU node, do:
## srun -p GPU --gres=gpu:XX:1 --pty tcsh
## where XX=v100 or k80, depending on which GPU you wish to go to.
## From there you might want to do:
## cd /state/partition1/ucapwhi/lfi_project/experiments

# Load the module files
source /share/splinter/ucapwhi/lfi_project/set_environment.csh USE

# Make directory for this experiment on the gpu node, if necessary
# Note that $experiment_name will have been set on the 'sbatch' command line
cd /state/partition1/
mkdir -p ucapwhi
cd ./ucapwhi
mkdir -p lfi_project
cd ./lfi_project
mkdir -p experiments
cd ./experiments
mkdir -p $experiment_name
cd ./$experiment_name

# Copy the control file from /share/splinter...
cp /share/splinter/ucapwhi/lfi_project/experiments/$experiment_name/control.par .

# Run pkdgrav3
/share/splinter/ucapwhi/lfi_project/pkdgrav3/build/pkdgrav3 -sz 9 ./control.par > ./example_output.txt

# Copy output files back to splinter
cp ./example* /share/splinter/ucapwhi/lfi_project/experiments/$experiment_name



