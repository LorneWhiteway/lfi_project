#!/bin/bash

#SBATCH -p GPU
#requesting one node
#SBATCH -N1
#requesting 1 cpu
#SBATCH -n1
#requesting 1 GPU
#SBATCH --gres=gpu:a100:1
#SBATCH --job-name=pkdgrav3
#SBATCH --mail-user=lorne.whiteway@star.ucl.ac.uk
#SBATCH --mail-type=END,FAIL
#SBATCH --time=0-01:00:00
#SBATCH --mem=36000


## Call this job script using syntax like this:
## sbatch --export=ALL,experiment_name='gpu_1024_1024_1000' cuda_job_script_hypatia

## To go to the GPU node, do:
## srun -p GPU --gres=gpu:a100:1 --pty bash
## From there you might want to do:
## cd /state/partition1/ucapwhi/lfi_project/experiments
## NOT sure if the above is correct for hypatia

# Load the module files
source /share/rcifdata/ucapwhi/lfi_project/set_environment_hypatia.sh USE


# Go to the output directory
cd /share/rcifdata/ucapwhi/lfi_project/experiments/$experiment_name/
	
# Run pkdgrav3
#/share/rcifdata/ucapwhi/lfi_project/pkdgrav3/build_hypatia_a100/pkdgrav3 -sz 32 ./control.par > ./output.txt
export OMP_NUM_THREADS=32
/share/rcifdata/ucapwhi/lfi_project/pkdgrav3/build_hypatia_a100/pkdgrav3 ./control.par > ./output.txt




