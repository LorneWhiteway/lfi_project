#!/bin/tcsh

#SBATCH -p GPU
#requesting one node
#SBATCH -N1
#requesting 12 cpus
#SBATCH -n12
#requesting 1 GPU
#SBATCH --gres=gpu:v100:1
#SBATCH --job-name=pkdgrav3
#SBATCH --mail-user=lorne.whiteway@star.ucl.ac.uk
#SBATCH --mail-type=END,FAIL
#SBATCH --time=40-00:00:00
#SBATCH --mem=128000


## Call this job script using syntax like this:
## sbatch --export=ALL,experiment_name='gpu_1024_1024_1000' cuda_job_script_splinter_v100

## To go to the GPU node, do:
## srun -p GPU --gres=gpu:v100:1 --pty tcsh
## From there you might want to do:
## cd /state/partition1/ucapwhi/lfi_project/experiments

# Load the module files
source /share/splinter/ucapwhi/lfi_project/set_environment_splinter.csh USE

if (0) then

	# This branch writes output files locally on gpu node, then copies them when done.

	# Make directory for this experiment on the gpu node, if necessary
	# Note that $experiment_name will have been set on the 'sbatch' command line
	cd /state/partition1/
	mkdir -p ucapwhi
	cd ./ucapwhi
	mkdir -p lfi_project
	cd ./lfi_project
	mkdir -p experiments
	cd ./experiments
	mkdir -p $experiment_name
	cd ./$experiment_name

	# Copy the control file from /share/splinter...
	cp /share/splinter/ucapwhi/lfi_project/experiments/$experiment_name/control.par .

	# Run pkdgrav3
	/share/splinter/ucapwhi/lfi_project/pkdgrav3/build_gpu_v100/pkdgrav3 -sz 32 ./control.par > ./output.txt

	# Copy output files back to splinter
	cp ./example* /share/splinter/ucapwhi/lfi_project/experiments/$experiment_name

else

	# This branch writes output files directly to splinter.

	# Go to the output directory
	cd /share/splinter/ucapwhi/lfi_project/experiments/$experiment_name/
	
	# Run pkdgrav3
	/share/splinter/ucapwhi/lfi_project/pkdgrav3/build_gpu_v100/pkdgrav3 -sz 32 ./control.par > ./output.txt


endif

