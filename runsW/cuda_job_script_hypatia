#!/bin/bash

#SBATCH -p GPU
#requesting one node
#SBATCH -N1
#requesting 1 cpu
#SBATCH -n1
#requesting 1 GPU
#SBATCH --gres=gpu:a100:1
#SBATCH --job-name=pkdgrav3
#SBATCH --mail-user=lorne.whiteway@star.ucl.ac.uk
#SBATCH --mail-type=END,FAIL
#SBATCH --time=0-01:00:00
#SBATCH --mem=36000


## Call this job script using syntax like this:
## sbatch --export=ALL,experiment_name='gpu_1024_1024_1000' cuda_job_script_hypatia

## To go to the GPU node, do:
## srun -p GPU --gres=gpu:a100:1 --pty bash
## From there you might want to do:
## cd /state/partition1/ucapwhi/lfi_project/experiments
## NOT sure if the above is correct for hypatia

# Load the module files
source /share/rcifdata/ucapwhi/lfi_project/set_environment_hypatia.sh USE


export OMP_NUM_THREADS=32

#! Full path to application executable: 
application="/share/rcifdata/ucapwhi/lfi_project/runsR/runXXX/pkdgrav3_and_post_process.sh"

#! Run options for the application:
#! Note that here we are assuming that the job file is being submitted from the directory
#! in which the control file is located.
options=""



#! Are you using OpenMP (NB this is unrelated to OpenMPI)? If so increase this
#! safe value to no more than 128:
export OMP_NUM_THREADS=32


#! Choose this for a pure shared-memory OpenMP parallel program on a single node:
#! (OMP_NUM_THREADS threads will be created):
CMD="$application $options"

#! Choose this for a MPI code using OpenMPI:
#CMD="mpirun -npernode $mpi_tasks_per_node -np $np $application $options"


echo -e "\nExecuting command:\n==================\n$CMD\n"

eval $CMD 








